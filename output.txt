--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\.github\workflows\hacs.yaml ---
name: HACS Action

on:
  push:
  pull_request:
  schedule:
    - cron: "0 0 * * *"

jobs:
  hacs:
    name: HACS Action
    runs-on: "ubuntu-latest"
    steps:
      - uses: "actions/checkout@v2"
      - name: HACS Action
        uses: "hacs/action@main"
        with:
          category: "integration"


--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\.github\workflows\hassfest.yaml ---
name: Validate with hassfest

on:
  push:
  pull_request:
  schedule:
    - cron: "0 0 * * *"

jobs:
  validate:
    runs-on: "ubuntu-latest"
    steps:
      - uses: "actions/checkout@v2"
      - uses: home-assistant/actions/hassfest@master


--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\.github\workflows\validate.yaml ---
name: Validate

on:
  push:
  pull_request:
  schedule:
    - cron: "0 0 * * *"
  workflow_dispatch:

jobs:
  validate-hacs:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"
      - name: HACS validation
        uses: hacs/action@main
        with:
          category: "integration"



--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\automations\new-entity-automation.yaml ---
########################################################################################
# AI Suggestions - New Entity Detection
########################################################################################
# This automation triggers the AI Automation Suggester whenever new entities are added to
# your Home Assistant instance.
#
# CUSTOMIZATION OPTIONS:
# 1. Trigger Events:
#    - Currently monitors both 'create' and 'update' events
#    - Remove the second trigger if you only want new entity detection
#
# 2. Throttling:
#    - Uses a cooldown of 1 hour to prevent excessive API calls
#    - Adjust the hours value in the condition template if needed
########################################################################################

alias: "AI Suggestions - New Entity Detection"
description: "Generates automation suggestions whenever new entities are registered in Home Assistant"

trigger:
  # Trigger when a new entity is created
  - platform: event
    event_type: entity_registry_updated
    event_data:
      action: create

  # Optional: Trigger when an entity is updated
  - platform: event
    event_type: entity_registry_updated
    event_data:
      action: update

condition:
  # Simple throttling based on last trigger time
  - condition: template
    value_template: >-
      {% set automation = states.automation.ai_suggestions_new_entity_detection %}
      {% if automation and automation.attributes.last_triggered %}
        {% set hours_since = ((now() - as_datetime(automation.attributes.last_triggered)).total_seconds() / 3600) | float %}
        {{ hours_since > 1.0 }}
      {% else %}
        true
      {% endif %}

action:
  - service: ai_automation_suggester.generate_suggestions
    target: {}
    data: {}


--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\automations\weekly-review-automation.yaml ---
########################################################################################
# AI Suggestions - Weekly System Review
########################################################################################
# This automation performs a comprehensive weekly review of your Home Assistant setup
# to suggest new automation opportunities and improvements.
#
# CUSTOMIZATION OPTIONS:
# 1. Schedule:
#    - Currently runs at 3 AM on Sundays
#    - Modify the 'at' field to change the time
#    - Change the weekday condition to run on different days
########################################################################################

alias: "AI Suggestions - Weekly Review"
description: "Performs a weekly scan of all entities to suggest new automation opportunities"

trigger:
  # Runs at 3 AM
  - platform: time
    at: "03:00:00"

condition:
  # Only runs on Sundays
  - condition: time
    weekday:
      - sun

action:
  # Generate suggestions using the configured provider(s), scanning all entities
  - service: ai_automation_suggester.generate_suggestions
    data:
      all_entities: true

  # Create a notification
  - service: persistent_notification.create
    data:
      title: "Weekly Automation Review"
      message: "The AI Automation Suggester has completed its weekly review. Check the suggestions sensor for new automation ideas!"
      notification_id: "weekly_automation_review"



--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\translations\de.json ---
{
    "config": {
      "step": {
        "user": {
          "title": "AI Automation Suggester konfigurieren",
          "description": "Wählen Sie Ihren KI-Anbieter und konfigurieren Sie die Einstellungen",
          "data": {
            "provider": "KI-Anbieter",
            "scan_frequency": "Scan-Häufigkeit (Stunden)",
            "initial_lag_time": "Anfangsverzögerung (Minuten)",
            "max_tokens": "Maximale Ausgabe-Token"
          }
        },
        "openai": {
          "title": "OpenAI Konfiguration",
          "data": {
            "api_key": "API-Schlüssel",
            "model": "Modell",
            "max_tokens": "Maximale Ausgabe-Token"
          },
          "description": "Die maximale Anzahl von Token bestimmt die Länge der Antwort der KI. Standard ist 500. Erhöhen Sie diesen Wert, wenn Sie längere Antworten benötigen."
        },
        "anthropic": {
          "title": "Anthropic Konfiguration",
          "data": {
            "api_key": "API-Schlüssel",
            "model": "Modell",
            "max_tokens": "Maximale Ausgabe-Token"
          },
          "description": "Die maximale Anzahl von Token bestimmt die Länge der Antwort der KI. Standard ist 500. Erhöhen Sie diesen Wert, wenn Sie längere Antworten benötigen."
        },
        "localai": {
          "title": "LocalAI Konfiguration",
          "data": {
            "ip_address": "IP-Adresse",
            "port": "Port",
            "https": "HTTPS verwenden",
            "model": "Modellname",
            "max_tokens": "Maximale Ausgabe-Token"
          },
          "description": "Die maximale Anzahl von Token bestimmt die Länge der Antwort der KI. Standard ist 500. Erhöhen Sie diesen Wert, wenn Sie längere Antworten benötigen."
        },
        "ollama": {
          "title": "Ollama Konfiguration",
          "data": {
            "ip_address": "IP-Adresse",
            "port": "Port",
            "https": "HTTPS verwenden",
            "model": "Modellname",
            "max_tokens": "Maximale Ausgabe-Token"
          },
          "description": "Die maximale Anzahl von Token bestimmt die Länge der Antwort der KI. Standard ist 500. Erhöhen Sie diesen Wert, wenn Sie längere Antworten benötigen."
        }
      },
      "error": {
        "cannot_connect": "Verbindung zum Dienst fehlgeschlagen",
        "invalid_auth": "Ungültige Authentifizierung",
        "invalid_config": "Ungültige Konfiguration",
        "unknown": "Unerwarteter Fehler",
        "no_entities": "Keine neuen Entitäten gefunden",
        "api_error": "API-Fehler aufgetreten",
        "required_field": "Dieses Feld ist erforderlich"
      }
    },
    "services": {
      "generate_suggestions": {
        "name": "Generiere Vorschläge",
        "description": "Löst manuell AI Automationsvorschläge aus",
        "fields": {
          "provider_config": {
            "name": "Anbieterkonfiguration",
            "description": "Welche Anbieterkonfiguration soll verwendet werden (falls mehrere vorhanden)?"
          },
          "custom_prompt": {
            "name": "Benutzerdefinierter Prompt",
            "description": "Optionaler benutzerdefinierter Prompt, um den Standard-System-Prompt zu überschreiben oder die Vorschläge auf bestimmte Themen auszurichten"
          },
          "all_entities": {
            "name": "Alle Entitäten berücksichtigen",
            "description": "Wenn wahr, werden alle Entitäten berücksichtigt, anstatt nur neue."
          },
          "domains": {
            "name": "Domänen",
            "description": "Liste der zu berücksichtigenden Domänen. Wenn leer, werden alle Domänen berücksichtigt."
          },
          "entity_limit": {
            "name": "Entitäten-Limit",
            "description": "Maximale Anzahl von Entitäten, die berücksichtigt werden (zufällig ausgewählt)."
          }
        }
      }
    }
  }
  


--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\translations\en.json ---
{
  "config": {
    "step": {
      "user": {
        "title": "Configure AI Automation Suggester",
        "data": {
          "provider": "AI Provider",
          "max_tokens": "Maximum Output Tokens"
        }
      },
      "provider_config": {
        "title": "Provider Settings",
        "data": {
          "model": "Model Name",
          "api_key": "API Key",
          "max_tokens": "Maximum Output Tokens",
          "ip_address": "IP Address",
          "port": "Port",
          "use_https": "Use HTTPS"
        },
        "description": "Maximum tokens controls the length of the AI's response. Default is 500. Increase if you need longer responses."
      }
    },
    "error": {
      "cannot_connect": "Failed to connect to service",
      "invalid_auth": "Invalid authentication",
      "invalid_config": "Invalid configuration",
      "unknown": "Unexpected error",
      "no_entities": "No new entities found",
      "api_error": "API error occurred",
      "required_field": "This field is required"
    },
    "abort": {
      "already_configured": "Provider is already configured",
      "provider_not_supported": "This provider is not currently supported"
    }
  },
  "services": {
    "generate_suggestions": {
      "name": "Generate Suggestions",
      "description": "Manually trigger AI automation suggestions",
      "fields": {
        "provider_config": {
          "name": "Provider Configuration",
          "description": "Which provider configuration to use (if you have multiple)"
        },
        "custom_prompt": {
          "name": "Custom Prompt",
          "description": "Optional custom prompt to override the default system prompt or guide the suggestions towards specific themes"
        },
        "all_entities": {
          "name": "Consider All Entities",
          "description": "If true, consider all entities instead of just new entities."
        },
        "domains": {
          "name": "Domains",
          "description": "List of domains to consider. If empty, consider all domains."
        },
        "entity_limit": {
          "name": "Entity Limit",
          "description": "Maximum number of entities to consider (randomly selected)."
        }
      }
    }
  }
}



--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\translations\it.json ---
{
  "services": {
    "generate_suggestions": {
      "name": "Genera Suggerimenti",
      "description": "Lancia manualmente suggerimenti di automazione AI",
      "fields": {
        "provider_config": {
          "name": "Configurazione Provider",
          "description": "Quale configurazione provider utilizzare (se ne hai più di una)?"
        },
        "custom_prompt": {
          "name": "Prompt Personalizzato",
          "description": "Prompt personalizzato opzionale per sovrascrivere il prompt di sistema predefinito o per focalizzare i suggerimenti su temi specifici (ad es. risparmio energetico)"
        },
        "all_entities": {
          "name": "Considera Tutte le Entità",
          "description": "Se vero, considera tutte le entità invece di solo quelle nuove."
        },
        "domains": {
          "name": "Domini",
          "description": "Elenco dei domini da considerare. Se vuoto, considera tutti i domini."
        },
        "entity_limit": {
          "name": "Limite di Entità",
          "description": "Numero massimo di entità da considerare (scelte casualmente)."
        }
      }
    }
  }
}



--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\translations\nl.json ---
{
  "services": {
    "generate_suggestions": {
      "name": "Genereer Suggesties",
      "description": "Handmatig AI-automatiseringssuggesties oproepen",
      "fields": {
        "provider_config": {
          "name": "Aanbiederconfiguratie",
          "description": "Welke aanbiederconfiguratie moet worden gebruikt (indien er meerdere zijn)?"
        },
        "custom_prompt": {
          "name": "Aangepaste Prompt",
          "description": "Optioneel een aangepaste prompt om de standaard systeemprompt te overschrijven of om de suggesties op specifieke thema's te richten (bijv. energiebesparing)"
        },
        "all_entities": {
          "name": "Alle Entiteiten Beschouwen",
          "description": "Indien waar, worden alle entiteiten in overweging genomen in plaats van alleen nieuwe entiteiten."
        },
        "domains": {
          "name": "Domeinen",
          "description": "Lijst met te beschouwen domeinen. Als deze leeg is, worden alle domeinen beschouwd."
        },
        "entity_limit": {
          "name": "Entiteitenlimiet",
          "description": "Maximaal aantal te beschouwen entiteiten (willekeurig geselecteerd)."
        }
      }
    }
  }
}



--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\translations\zh.json ---
{
  "services": {
    "generate_suggestions": {
      "name": "生成建议",
      "description": "手动触发 AI 自动化建议",
      "fields": {
        "provider_config": {
          "name": "提供商配置",
          "description": "选择要使用的提供商配置（如果有多个）"
        },
        "custom_prompt": {
          "name": "自定义提示",
          "description": "可选的自定义提示，可覆盖默认系统提示或将建议引导至特定主题（例如节能自动化）"
        },
        "all_entities": {
          "name": "考虑所有实体",
          "description": "若为 true，则考虑所有实体而不仅仅是新实体。"
        },
        "domains": {
          "name": "领域",
          "description": "要考虑的领域列表。如果为空，则考虑所有领域。"
        },
        "entity_limit": {
          "name": "实体限制",
          "description": "要考虑的实体最大数量（随机选择）。"
        }
      }
    }
  }
}



--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\config_flow.py ---
# custom_components/ai_automation_suggester/config_flow.py

"""Config flow for AI Automation Suggester integration."""
import logging
import voluptuous as vol
from typing import Any, Dict, Optional

from homeassistant import config_entries
from homeassistant.core import callback
from homeassistant.exceptions import ServiceValidationError
from homeassistant.helpers.aiohttp_client import async_get_clientsession

from .const import (
    DOMAIN,
    CONF_PROVIDER,
    CONF_MAX_TOKENS,
    DEFAULT_MAX_TOKENS,
    CONF_OPENAI_API_KEY,
    CONF_OPENAI_MODEL,
    CONF_ANTHROPIC_API_KEY,
    CONF_ANTHROPIC_MODEL,
    CONF_GOOGLE_API_KEY,
    CONF_GOOGLE_MODEL,
    CONF_GROQ_API_KEY,
    CONF_GROQ_MODEL,
    CONF_LOCALAI_IP_ADDRESS,
    CONF_LOCALAI_PORT,
    CONF_LOCALAI_HTTPS,
    CONF_LOCALAI_MODEL,
    CONF_OLLAMA_IP_ADDRESS,
    CONF_OLLAMA_PORT,
    CONF_OLLAMA_HTTPS,
    CONF_OLLAMA_MODEL,
    CONF_CUSTOM_OPENAI_ENDPOINT,
    CONF_CUSTOM_OPENAI_API_KEY,
    CONF_CUSTOM_OPENAI_MODEL,
    DEFAULT_MODELS,
    VERSION_ANTHROPIC,
    # Mistral AI additions:
    CONF_MISTRAL_API_KEY,
    CONF_MISTRAL_MODEL,
)

_LOGGER = logging.getLogger(__name__)


class ProviderValidator:
    """Validate provider configurations."""

    def __init__(self, hass):
        self.hass = hass
        self.session = async_get_clientsession(hass)

    async def validate_openai(self, api_key: str) -> Optional[str]:
        headers = {
            'Authorization': f"Bearer {api_key}",
            'Content-Type': 'application/json',
        }
        try:
            _LOGGER.debug("Validating OpenAI API key")
            response = await self.session.get("https://api.openai.com/v1/models", headers=headers)
            if response.status == 200:
                return None  # Success
            else:
                error_text = await response.text()
                _LOGGER.error(f"OpenAI validation error response: {error_text}")
                try:
                    error_json = await response.json()
                    error_message = error_json.get("error", {}).get("message", error_text)
                except Exception:
                    error_message = error_text
                return error_message
        except Exception as err:
            _LOGGER.error(f"OpenAI validation exception: {err}")
            return str(err)

    async def validate_anthropic(self, api_key: str, model: str) -> Optional[str]:
        headers = {
            'x-api-key': api_key,
            'anthropic-version': VERSION_ANTHROPIC,
            'content-type': 'application/json'
        }
        try:
            _LOGGER.debug("Validating Anthropic API key")
            response = await self.session.post(
                "https://api.anthropic.com/v1/complete",
                headers=headers,
                json={
                    "prompt": "\n\nHuman: Hello\n\nAssistant:",
                    "model": model,
                    "max_tokens_to_sample": 1,
                    "temperature": 0.5,
                    "stop_sequences": ["\n\nHuman:"]
                }
            )
            if response.status == 200:
                return None  # Success
            else:
                error_text = await response.text()
                _LOGGER.error(f"Anthropic validation error response: {error_text}")
                try:
                    error_json = await response.json()
                    error_message = error_json.get("error", {}).get("message", error_text)
                except Exception:
                    error_message = error_text
                return error_message
        except Exception as err:
            _LOGGER.error(f"Anthropic validation exception: {err}")
            return str(err)

    async def validate_google(self, api_key: str, model: str) -> Optional[str]:
        headers = {'Content-Type': 'application/json'}
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
        payload = {
            "contents": [{"parts": [{"text": "Hello"}]}],
            "generationConfig": {"temperature": 0.5, "maxOutputTokens": 100, "topK": 40, "topP": 0.95}
        }
        try:
            _LOGGER.debug(f"Validating Google API key with model: {model}")
            response = await self.session.post(url, headers=headers, json=payload)
            if response.status == 200:
                return None  # Success
            else:
                error_text = await response.text()
                _LOGGER.error(f"Google validation error response: {error_text}")
                try:
                    error_json = await response.json()
                    error_message = error_json.get("error", {}).get("message", error_text)
                except Exception:
                    error_message = error_text
                return error_message
        except Exception as err:
            _LOGGER.error(f"Google validation exception: {err}")
            return str(err)

    async def validate_groq(self, api_key: str) -> Optional[str]:
        headers = {'Authorization': f"Bearer {api_key}", 'Content-Type': 'application/json'}
        try:
            _LOGGER.debug("Validating Groq API key")
            response = await self.session.get("https://api.groq.com/openai/v1/models", headers=headers)
            if response.status == 200:
                return None  # Success
            else:
                error_text = await response.text()
                _LOGGER.error(f"Groq validation error response: {error_text}")
                try:
                    error_json = await response.json()
                    error_message = error_json.get("error", {}).get("message", error_text)
                except Exception:
                    error_message = error_text
                return error_message
        except Exception as err:
            _LOGGER.error(f"Groq validation exception: {err}")
            return str(err)

    async def validate_localai(self, ip_address: str, port: int, https: bool = False) -> Optional[str]:
        protocol = "https" if https else "http"
        url = f"{protocol}://{ip_address}:{port}/v1/models"
        try:
            _LOGGER.debug(f"Validating LocalAI connection to {url}")
            response = await self.session.get(url)
            if response.status == 200:
                return None  # Success
            else:
                error_text = await response.text()
                _LOGGER.error(f"LocalAI validation error response: {error_text}")
                return f"HTTP {response.status}: {error_text}"
        except Exception as err:
            _LOGGER.error(f"LocalAI validation exception: {err}")
            return str(err)

    async def validate_ollama(self, ip_address: str, port: int, https: bool = False) -> Optional[str]:
        protocol = "https" if https else "http"
        url = f"{protocol}://{ip_address}:{port}/api/tags"
        try:
            _LOGGER.debug(f"Validating Ollama connection to {url}")
            response = await self.session.get(url)
            if response.status == 200:
                return None  # Success
            else:
                error_text = await response.text()
                _LOGGER.error(f"Ollama validation error response: {error_text}")
                return f"HTTP {response.status}: {error_text}"
        except Exception as err:
            _LOGGER.error(f"Ollama validation exception: {err}")
            return str(err)

    async def validate_custom_openai(self, endpoint: str, api_key: Optional[str]) -> Optional[str]:
        headers = {'Content-Type': 'application/json'}
        if api_key:
            headers['Authorization'] = f"Bearer {api_key}"
        try:
            _LOGGER.debug(f"Validating Custom OpenAI endpoint {endpoint}")
            response = await self.session.get(f"{endpoint}/v1/models", headers=headers)
            if response.status == 200:
                return None  # Success
            else:
                error_text = await response.text()
                _LOGGER.error(f"Custom OpenAI validation error response: {error_text}")
                try:
                    error_json = await response.json()
                    error_message = error_json.get("error", {}).get("message", error_text)
                except Exception:
                    error_message = error_text
                return error_message
        except Exception as err:
            _LOGGER.error(f"Custom OpenAI validation exception: {err}")
            return str(err)


class AIAutomationConfigFlow(config_entries.ConfigFlow, domain=DOMAIN):
    """Handle a config flow for AI Automation Suggester integration."""

    VERSION = 1

    def __init__(self):
        self.provider = None
        self.data = {}
        self.validator = None

    @staticmethod
    @callback
    def async_get_options_flow(config_entry):
        """Get the options flow for this handler."""
        return AIAutomationOptionsFlowHandler(config_entry)

    async def async_step_user(self, user_input: Optional[Dict[str, Any]] = None):
        errors = {}
        if user_input is not None:
            self.provider = user_input[CONF_PROVIDER]
            self.data.update(user_input)
            # Check if provider is already configured
            existing_entries = self._async_current_entries()
            for entry in existing_entries:
                if entry.data.get(CONF_PROVIDER) == self.provider:
                    errors["base"] = "already_configured"
                    break
            if not errors:
                provider_steps = {
                    "OpenAI": self.async_step_openai,
                    "Anthropic": self.async_step_anthropic,
                    "Google": self.async_step_google,
                    "Groq": self.async_step_groq,
                    "LocalAI": self.async_step_localai,
                    "Ollama": self.async_step_ollama,
                    "Custom OpenAI": self.async_step_custom_openai,
                    "Mistral AI": self.async_step_mistral,
                }
                return await provider_steps[self.provider]()
        providers = ["OpenAI", "Anthropic", "Google", "Groq", "LocalAI", "Ollama", "Custom OpenAI", "Mistral AI"]
        return self.async_show_form(
            step_id="user",
            data_schema=vol.Schema({vol.Required(CONF_PROVIDER): vol.In(providers)}),
            errors=errors
        )

    async def async_step_openai(self, user_input: Optional[Dict[str, Any]] = None):
        errors = {}
        description_placeholders = {}
        if user_input is not None:
            self.validator = ProviderValidator(self.hass)
            error_message = await self.validator.validate_openai(user_input[CONF_OPENAI_API_KEY])
            if error_message is None:
                self.data.update(user_input)
                return self.async_create_entry(title="AI Automation Suggester (OpenAI)", data=self.data)
            else:
                errors["base"] = "api_error"
                description_placeholders["error_message"] = error_message
        return self.async_show_form(
            step_id="openai",
            data_schema=vol.Schema({
                vol.Required(CONF_OPENAI_API_KEY): str,
                vol.Optional(CONF_OPENAI_MODEL, default=DEFAULT_MODELS["OpenAI"]): str,
                vol.Optional(CONF_MAX_TOKENS, default=DEFAULT_MAX_TOKENS):
                    vol.All(vol.Coerce(int), vol.Range(min=100)),
            }),
            errors=errors,
            description_placeholders=description_placeholders
        )

    async def async_step_anthropic(self, user_input: Optional[Dict[str, Any]] = None):
        errors = {}
        description_placeholders = {}
        if user_input is not None:
            self.validator = ProviderValidator(self.hass)
            model = user_input.get(CONF_ANTHROPIC_MODEL, DEFAULT_MODELS["Anthropic"])
            error_message = await self.validator.validate_anthropic(api_key=user_input[CONF_ANTHROPIC_API_KEY], model=model)
            if error_message is None:
                self.data.update(user_input)
                return self.async_create_entry(title="AI Automation Suggester (Anthropic)", data=self.data)
            else:
                errors["base"] = "api_error"
                description_placeholders["error_message"] = error_message
        return self.async_show_form(
            step_id="anthropic",
            data_schema=vol.Schema({
                vol.Required(CONF_ANTHROPIC_API_KEY): str,
                vol.Optional(CONF_ANTHROPIC_MODEL, default=DEFAULT_MODELS["Anthropic"]): str,
                vol.Optional(CONF_MAX_TOKENS, default=DEFAULT_MAX_TOKENS):
                    vol.All(vol.Coerce(int), vol.Range(min=100)),
            }),
            errors=errors,
            description_placeholders=description_placeholders
        )

    async def async_step_google(self, user_input: Optional[Dict[str, Any]] = None):
        errors = {}
        description_placeholders = {}
        if user_input is not None:
            self.validator = ProviderValidator(self.hass)
            model = user_input.get(CONF_GOOGLE_MODEL, DEFAULT_MODELS["Google"])
            error_message = await self.validator.validate_google(api_key=user_input[CONF_GOOGLE_API_KEY], model=model)
            if error_message is None:
                self.data.update(user_input)
                return self.async_create_entry(title="AI Automation Suggester (Google)", data=self.data)
            else:
                errors["base"] = "api_error"
                description_placeholders["error_message"] = error_message
        return self.async_show_form(
            step_id="google",
            data_schema=vol.Schema({
                vol.Required(CONF_GOOGLE_API_KEY): str,
                vol.Optional(CONF_GOOGLE_MODEL, default=DEFAULT_MODELS["Google"]): str,
                vol.Optional(CONF_MAX_TOKENS, default=DEFAULT_MAX_TOKENS):
                    vol.All(vol.Coerce(int), vol.Range(min=100)),
            }),
            errors=errors,
            description_placeholders=description_placeholders
        )

    async def async_step_groq(self, user_input: Optional[Dict[str, Any]] = None):
        errors = {}
        description_placeholders = {}
        if user_input is not None:
            self.validator = ProviderValidator(self.hass)
            error_message = await self.validator.validate_groq(user_input[CONF_GROQ_API_KEY])
            if error_message is None:
                self.data.update(user_input)
                return self.async_create_entry(title="AI Automation Suggester (Groq)", data=self.data)
            else:
                errors["base"] = "api_error"
                description_placeholders["error_message"] = error_message
        return self.async_show_form(
            step_id="groq",
            data_schema=vol.Schema({
                vol.Required(CONF_GROQ_API_KEY): str,
                vol.Optional(CONF_GROQ_MODEL, default=DEFAULT_MODELS["Groq"]): str,
                vol.Optional(CONF_MAX_TOKENS, default=DEFAULT_MAX_TOKENS):
                    vol.All(vol.Coerce(int), vol.Range(min=100)),
            }),
            errors=errors,
            description_placeholders=description_placeholders
        )

    async def async_step_localai(self, user_input: Optional[Dict[str, Any]] = None):
        errors = {}
        description_placeholders = {}
        if user_input is not None:
            self.validator = ProviderValidator(self.hass)
            error_message = await self.validator.validate_localai(
                user_input[CONF_LOCALAI_IP_ADDRESS],
                user_input[CONF_LOCALAI_PORT],
                user_input[CONF_LOCALAI_HTTPS]
            )
            if error_message is None:
                self.data.update(user_input)
                return self.async_create_entry(title="AI Automation Suggester (LocalAI)", data=self.data)
            else:
                errors["base"] = "api_error"
                description_placeholders["error_message"] = error_message
        return self.async_show_form(
            step_id="localai",
            data_schema=vol.Schema({
                vol.Required(CONF_LOCALAI_IP_ADDRESS): str,
                vol.Required(CONF_LOCALAI_PORT, default=8080): int,
                vol.Required(CONF_LOCALAI_HTTPS, default=False): bool,
                vol.Optional(CONF_LOCALAI_MODEL, default=DEFAULT_MODELS["LocalAI"]): str,
                vol.Optional(CONF_MAX_TOKENS, default=DEFAULT_MAX_TOKENS):
                    vol.All(vol.Coerce(int), vol.Range(min=100)),
            }),
            errors=errors,
            description_placeholders=description_placeholders
        )

    async def async_step_ollama(self, user_input: Optional[Dict[str, Any]] = None):
        errors = {}
        description_placeholders = {}
        if user_input is not None:
            self.validator = ProviderValidator(self.hass)
            error_message = await self.validator.validate_ollama(
                user_input[CONF_OLLAMA_IP_ADDRESS],
                user_input[CONF_OLLAMA_PORT],
                user_input[CONF_OLLAMA_HTTPS]
            )
            if error_message is None:
                self.data.update(user_input)
                return self.async_create_entry(title="AI Automation Suggester (Ollama)", data=self.data)
            else:
                errors["base"] = "api_error"
                description_placeholders["error_message"] = error_message
        return self.async_show_form(
            step_id="ollama",
            data_schema=vol.Schema({
                vol.Required(CONF_OLLAMA_IP_ADDRESS): str,
                vol.Required(CONF_OLLAMA_PORT, default=11434): int,
                vol.Required(CONF_OLLAMA_HTTPS, default=False): bool,
                vol.Optional(CONF_OLLAMA_MODEL, default=DEFAULT_MODELS["Ollama"]): str,
                vol.Optional(CONF_MAX_TOKENS, default=DEFAULT_MAX_TOKENS):
                    vol.All(vol.Coerce(int), vol.Range(min=100)),
            }),
            errors=errors,
            description_placeholders=description_placeholders
        )

    async def async_step_custom_openai(self, user_input: Optional[Dict[str, Any]] = None):
        errors = {}
        description_placeholders = {}
        if user_input is not None:
            self.validator = ProviderValidator(self.hass)
            api_key = user_input.get(CONF_CUSTOM_OPENAI_API_KEY)
            endpoint = user_input[CONF_CUSTOM_OPENAI_ENDPOINT]
            error_message = await self.validator.validate_custom_openai(endpoint=endpoint, api_key=api_key)
            if error_message is None:
                self.data.update(user_input)
                return self.async_create_entry(title="AI Automation Suggester (Custom OpenAI)", data=self.data)
            else:
                errors["base"] = "api_error"
                description_placeholders["error_message"] = error_message
        return self.async_show_form(
            step_id="custom_openai",
            data_schema=vol.Schema({
                vol.Required(CONF_CUSTOM_OPENAI_ENDPOINT): str,
                vol.Optional(CONF_CUSTOM_OPENAI_API_KEY): str,
                vol.Optional(CONF_CUSTOM_OPENAI_MODEL, default=DEFAULT_MODELS["Custom OpenAI"]): str,
                vol.Optional(CONF_MAX_TOKENS, default=DEFAULT_MAX_TOKENS):
                    vol.All(vol.Coerce(int), vol.Range(min=100)),
            }),
            errors=errors,
            description_placeholders=description_placeholders
        )

    async def async_step_mistral(self, user_input: Optional[Dict[str, Any]] = None):
        errors = {}
        if user_input is not None:
            self.data.update(user_input)
            return self.async_create_entry(title="AI Automation Suggester (Mistral AI)", data=self.data)
        return self.async_show_form(
            step_id="mistral",
            data_schema=vol.Schema({
                vol.Required(CONF_MISTRAL_API_KEY): str,
                vol.Required(CONF_MISTRAL_MODEL, default="mistral-large-latest"): str,
                vol.Optional(CONF_MAX_TOKENS, default=DEFAULT_MAX_TOKENS):
                    vol.All(vol.Coerce(int), vol.Range(min=100)),
            }),
            errors=errors
        )


class AIAutomationOptionsFlowHandler(config_entries.OptionsFlow):
    """Handle options for the AI Automation Suggester."""

    async def async_step_init(self, user_input: Optional[Dict[str, Any]] = None):
        if user_input is not None:
            return self.async_create_entry(title="", data=user_input)
        provider = self.config_entry.data.get(CONF_PROVIDER)
        options = {
            vol.Optional(
                CONF_MAX_TOKENS,
                default=self.config_entry.data.get(CONF_MAX_TOKENS, DEFAULT_MAX_TOKENS)
            ): vol.All(vol.Coerce(int), vol.Range(min=100))
        }
        if provider == "OpenAI":
            options[vol.Optional(CONF_OPENAI_API_KEY)] = str
            options[vol.Optional(CONF_OPENAI_MODEL, default=self.config_entry.data.get(CONF_OPENAI_MODEL, DEFAULT_MODELS["OpenAI"]))] = str
        elif provider == "Anthropic":
            options[vol.Optional(CONF_ANTHROPIC_API_KEY)] = str
            options[vol.Optional(CONF_ANTHROPIC_MODEL, default=self.config_entry.data.get(CONF_ANTHROPIC_MODEL, DEFAULT_MODELS["Anthropic"]))] = str
        elif provider == "Google":
            options[vol.Optional(CONF_GOOGLE_API_KEY)] = str
            options[vol.Optional(CONF_GOOGLE_MODEL, default=self.config_entry.data.get(CONF_GOOGLE_MODEL, DEFAULT_MODELS["Google"]))] = str
        elif provider == "Groq":
            options[vol.Optional(CONF_GROQ_API_KEY)] = str
            options[vol.Optional(CONF_GROQ_MODEL, default=self.config_entry.data.get(CONF_GROQ_MODEL, DEFAULT_MODELS["Groq"]))] = str
        elif provider == "LocalAI":
            options[vol.Optional(CONF_LOCALAI_HTTPS)] = bool
            options[vol.Optional(CONF_LOCALAI_MODEL, default=self.config_entry.data.get(CONF_LOCALAI_MODEL, DEFAULT_MODELS["LocalAI"]))] = str
        elif provider == "Ollama":
            options[vol.Optional(CONF_OLLAMA_HTTPS)] = bool
            options[vol.Optional(CONF_OLLAMA_MODEL, default=self.config_entry.data.get(CONF_OLLAMA_MODEL, DEFAULT_MODELS["Ollama"]))] = str
        elif provider == "Custom OpenAI":
            options[vol.Optional(CONF_CUSTOM_OPENAI_ENDPOINT)] = str
            options[vol.Optional(CONF_CUSTOM_OPENAI_API_KEY)] = str
            options[vol.Optional(CONF_CUSTOM_OPENAI_MODEL, default=self.config_entry.data.get(CONF_CUSTOM_OPENAI_MODEL, DEFAULT_MODELS["Custom OpenAI"]))] = str
        elif provider == "Mistral AI":
            options[vol.Required(CONF_MISTRAL_API_KEY)] = str
            options[vol.Required(CONF_MISTRAL_MODEL, default=self.config_entry.data.get(CONF_MISTRAL_MODEL, "mistral-large-latest"))] = str

        return self.async_show_form(
            step_id="init",
            data_schema=vol.Schema(options)
        )



--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\const.py ---
# custom_components/ai_automation_suggester/const.py

"""Constants for the AI Automation Suggester integration."""

DOMAIN = "ai_automation_suggester"
PLATFORMS = ["sensor"]

# Config version
CONFIG_VERSION = 2

# Provider configuration
CONF_PROVIDER = "provider"

# OpenAI specific
CONF_OPENAI_API_KEY = "openai_api_key"
CONF_OPENAI_MODEL = "openai_model"

# Anthropic specific
CONF_ANTHROPIC_API_KEY = "anthropic_api_key"
CONF_ANTHROPIC_MODEL = "anthropic_model"
VERSION_ANTHROPIC = "2023-06-01"

# Google specific
CONF_GOOGLE_API_KEY = "google_api_key"
CONF_GOOGLE_MODEL = "google_model"

# Groq specific
CONF_GROQ_API_KEY = "groq_api_key"
CONF_GROQ_MODEL = "groq_model"

# LocalAI specific
CONF_LOCALAI_IP_ADDRESS = "localai_ip"
CONF_LOCALAI_PORT = "localai_port"
CONF_LOCALAI_HTTPS = "localai_https"
CONF_LOCALAI_MODEL = "localai_model"

# Ollama specific
CONF_OLLAMA_IP_ADDRESS = "ollama_ip"
CONF_OLLAMA_PORT = "ollama_port"
CONF_OLLAMA_HTTPS = "ollama_https"
CONF_OLLAMA_MODEL = "ollama_model"

# Custom OpenAI specific
CONF_CUSTOM_OPENAI_ENDPOINT = "custom_openai_endpoint"
CONF_CUSTOM_OPENAI_API_KEY = "custom_openai_api_key"
CONF_CUSTOM_OPENAI_MODEL = "custom_openai_model"

# Mistral AI specific (new)
CONF_MISTRAL_API_KEY = "mistral_api_key"
CONF_MISTRAL_MODEL = "mistral_model"
MISTRAL_MODELS = ["mistral-tiny", "mistral-small", "mistral-medium", "mistral-large"]

# Model Defaults
DEFAULT_MODELS = {
    "OpenAI": "gpt-4o-mini",
    "Anthropic": "claude-2.1",
    "Google": "gemini-1.0-pro",
    "Groq": "llama3-8b-8192",
    "LocalAI": "llama3",
    "Ollama": "llama2",
    "Custom OpenAI": "gpt-3.5-turbo",
    "Mistral AI": "mistral-medium"
}

# Error Messages
ERROR_INVALID_API_KEY = "Invalid API key"
ERROR_CONNECTION_FAILED = "Could not connect to server"
ERROR_INVALID_CONFIG = "Invalid configuration"
ERROR_OPENAI_NOT_CONFIGURED = "OpenAI is not configured"
ERROR_ANTHROPIC_NOT_CONFIGURED = "Anthropic is not configured"
ERROR_GOOGLE_NOT_CONFIGURED = "Google is not configured"
ERROR_GROQ_NOT_CONFIGURED = "Groq is not configured"
ERROR_GROQ_MULTIPLE_IMAGES = "Groq does not support videos or streams"
ERROR_LOCALAI_NOT_CONFIGURED = "LocalAI is not configured"
ERROR_OLLAMA_NOT_CONFIGURED = "Ollama is not configured"
ERROR_CUSTOM_OPENAI_NOT_CONFIGURED = "Custom OpenAI provider is not configured"
ERROR_NO_IMAGE_INPUT = "No image input provided"
ERROR_HANDSHAKE_FAILED = "Connection could not be established"

# Service attributes
ATTR_PROVIDER_CONFIG = "provider_config"
ATTR_CUSTOM_PROMPT = "custom_prompt"

SERVICE_GENERATE_SUGGESTIONS = "generate_suggestions"

# Provider statuses
PROVIDER_STATUS_CONNECTED = "connected"
PROVIDER_STATUS_DISCONNECTED = "disconnected"
PROVIDER_STATUS_ERROR = "error"

# Event types
EVENT_NEW_SUGGESTION = f"{DOMAIN}_new_suggestion"
EVENT_PROVIDER_STATUS_CHANGE = f"{DOMAIN}_provider_status_change"

# Configuration defaults
CONF_MAX_TOKENS = "max_tokens"
DEFAULT_MAX_TOKENS = 500  # Conservative default; adjust as needed per provider
DEFAULT_TEMPERATURE = 0.7

# API Endpoints
ENDPOINT_OPENAI = "https://api.openai.com/v1/chat/completions"
ENDPOINT_ANTHROPIC = "https://api.anthropic.com/v1/messages"
ENDPOINT_GOOGLE = "https://generativelanguage.googleapis.com/v1beta2/models/{model}:generateText?key={api_key}"
ENDPOINT_GROQ = "https://api.groq.com/openai/v1/chat/completions"
ENDPOINT_LOCALAI = "{protocol}://{ip_address}:{port}/v1/chat/completions"
ENDPOINT_OLLAMA = "{protocol}://{ip_address}:{port}/api/chat"
ENDPOINT_MISTRAL = "https://api.mistral.ai/v1/chat/completions"


--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\coordinator.py ---
# custom_components/ai_automation_suggester/coordinator.py

"""Coordinator for AI Automation Suggester."""
import logging
import random
from datetime import datetime
from homeassistant.components import persistent_notification
from homeassistant.core import HomeAssistant
from homeassistant.helpers.update_coordinator import DataUpdateCoordinator
from homeassistant.helpers.aiohttp_client import async_get_clientsession
from homeassistant.helpers import device_registry as dr, entity_registry as er, area_registry as ar

from .const import (
    DOMAIN,
    CONF_PROVIDER,
    DEFAULT_MODELS,
    CONF_MAX_TOKENS,
    CONF_OPENAI_API_KEY,
    CONF_OPENAI_MODEL,
    CONF_ANTHROPIC_API_KEY,
    CONF_ANTHROPIC_MODEL,
    CONF_GOOGLE_API_KEY,
    CONF_GOOGLE_MODEL,
    CONF_GROQ_API_KEY,
    CONF_GROQ_MODEL,
    CONF_LOCALAI_IP_ADDRESS,
    CONF_LOCALAI_PORT,
    CONF_LOCALAI_HTTPS,
    CONF_LOCALAI_MODEL,
    CONF_OLLAMA_IP_ADDRESS,
    CONF_OLLAMA_PORT,
    CONF_OLLAMA_HTTPS,
    CONF_OLLAMA_MODEL,
    CONF_CUSTOM_OPENAI_ENDPOINT,
    CONF_CUSTOM_OPENAI_API_KEY,
    CONF_CUSTOM_OPENAI_MODEL,
    # Mistral AI constants added:
    CONF_MISTRAL_API_KEY,
    CONF_MISTRAL_MODEL,
    DEFAULT_MAX_TOKENS,
    DEFAULT_TEMPERATURE,
    VERSION_ANTHROPIC,
    ENDPOINT_OPENAI,
    ENDPOINT_ANTHROPIC,
    ENDPOINT_GOOGLE,
    ENDPOINT_GROQ,
    ENDPOINT_LOCALAI,
    ENDPOINT_OLLAMA,
    # New Mistral AI endpoint constant:
    ENDPOINT_MISTRAL,
)

_LOGGER = logging.getLogger(__name__)

SYSTEM_PROMPT = """You are an AI assistant that generates Home Assistant automations 
based on the types of entities, their areas, and their associated devices, as well as 
improving or suggesting new automations based on existing ones.

For each entity:
1. Understand its function, area, and device context.
2. Consider its current state and attributes.
3. Suggest contextually aware automations and improvements to existing automations.
4. Include actual entity IDs in your suggestions.

When focusing on custom aspects (like energy-saving or presence-based lighting), 
integrate those themes into the automations. Provide triggers, conditions, 
and detailed actions to refine the automations according to the instructions given in the custom prompt.

Also consider existing automations and how they can be improved or complemented.
"""

class AIAutomationCoordinator(DataUpdateCoordinator):
    """Class to manage fetching data from AI model."""

    def __init__(self, hass: HomeAssistant, entry) -> None:
        """Initialize."""
        self.hass = hass
        self.entry = entry
        self.previous_entities = {}
        self.last_update = None
        self.SYSTEM_PROMPT = SYSTEM_PROMPT
        self.scan_all = False      # Flag to consider all entities
        self.selected_domains = [] # Filter to specified domains if provided
        self.entity_limit = 200    # Limit number of entities considered

        self.data = {
            "suggestions": "No suggestions yet",
            "last_update": None,
            "entities_processed": [],
            "provider": entry.data.get(CONF_PROVIDER, "unknown")
        }

        # Prevent automatic updates by setting update_interval to None
        self.update_interval = None
        self.session = async_get_clientsession(hass)

        self.device_registry = None
        self.entity_registry = None
        self.area_registry = None

        super().__init__(hass, _LOGGER, name=DOMAIN, update_interval=self.update_interval)

    async def async_added_to_hass(self):
        await super().async_added_to_hass()
        self.device_registry = dr.async_get(self.hass)
        self.entity_registry = er.async_get(self.hass)
        self.area_registry = ar.async_get(self.hass)

    async def _async_update_data(self):
        try:
            current_time = datetime.now()
            _LOGGER.debug("Starting manual update at %s", current_time)

            self.last_update = current_time

            # Fetch current entities
            _LOGGER.debug("Fetching current entities")
            try:
                current_entities = {}
                for entity_id in self.hass.states.async_entity_ids():
                    # If user provided domains to filter, only include those domains
                    if self.selected_domains:
                        domain = entity_id.split('.')[0]
                        if domain not in self.selected_domains:
                            continue

                    state = self.hass.states.get(entity_id)
                    if state is not None:
                        friendly_name = state.attributes.get('friendly_name', entity_id)
                        current_entities[entity_id] = {
                            'state': state.state,
                            'attributes': state.attributes,
                            'last_changed': state.last_changed,
                            'last_updated': state.last_updated,
                            'friendly_name': friendly_name
                        }
            except Exception as err:
                _LOGGER.error("Error fetching entities: %s", err)
                return self.data

            if self.scan_all:
                selected_entities = current_entities
            else:
                # Only consider new entities if scan_all is False
                selected_entities = {
                    k: v for k, v in current_entities.items()
                    if k not in self.previous_entities
                }

            if not selected_entities:
                _LOGGER.debug("No entities selected for suggestions")
                self.previous_entities = current_entities
                return self.data

            ai_input_data = self.prepare_ai_input(selected_entities)
            suggestions = await self.get_ai_suggestions(ai_input_data)

            if suggestions:
                _LOGGER.debug("Received suggestions: %s", suggestions)
                try:
                    persistent_notification.async_create(
                        self.hass,
                        message=suggestions,
                        title="AI Automation Suggestions",
                        notification_id=f"ai_automation_suggestions_{current_time.timestamp()}"
                    )

                    self.data = {
                        "suggestions": suggestions,
                        "last_update": current_time,
                        "entities_processed": list(selected_entities.keys()),
                        "provider": self.entry.data.get(CONF_PROVIDER, "unknown")
                    }

                    await self.hass.services.async_call(
                        "logbook", 
                        "log", 
                        {"name": "AI Automation Suggester", "message": "New suggestions generated"}
                    )

                except Exception as err:
                    _LOGGER.error("Error creating notification: %s", err)
                    self.data = {
                        "suggestions": suggestions,
                        "last_update": current_time,
                        "entities_processed": list(selected_entities.keys()),
                        "provider": self.entry.data.get(CONF_PROVIDER, "unknown")
                    }
            else:
                _LOGGER.warning("No valid suggestions received from AI")
                self.data = {
                    "suggestions": "No suggestions available",
                    "last_update": current_time,
                    "entities_processed": [],
                    "provider": self.entry.data.get(CONF_PROVIDER, "unknown")
                }

            self.previous_entities = current_entities
            return self.data

        except Exception as err:
            _LOGGER.error("Unexpected error in update: %s", err)
            return self.data

    def prepare_ai_input(self, entities):
        _LOGGER.debug("Preparing AI input for %d entities", len(entities))

        MAX_ATTR_LENGTH = 500
        MAX_AUTOMATIONS = 100

        # Randomly pick entities up to entity_limit
        entity_list = list(entities.items())
        selected_entities = random.sample(entity_list, min(len(entity_list), self.entity_limit))

        entities_description = []
        for entity_id, entity_data in selected_entities:
            state = entity_data.get('state', 'unknown')
            attributes = entity_data.get('attributes', {})
            friendly_name = entity_data.get('friendly_name', entity_id)
            domain = entity_id.split('.')[0]

            attr_str = str(attributes)
            if len(attr_str) > MAX_ATTR_LENGTH:
                attr_str = attr_str[:MAX_ATTR_LENGTH] + "...(truncated)"

            ent_reg_entry = self.entity_registry.async_get(entity_id) if self.entity_registry else None
            device_info = None
            area_name = "Unknown Area"
            if ent_reg_entry:
                if ent_reg_entry.device_id and self.device_registry:
                    dev_reg_entry = self.device_registry.async_get(ent_reg_entry.device_id)
                    if dev_reg_entry:
                        device_info = {
                            "manufacturer": dev_reg_entry.manufacturer,
                            "model": dev_reg_entry.model,
                            "name": dev_reg_entry.name_by_user or dev_reg_entry.name,
                            "id": dev_reg_entry.id
                        }

                area_id = ent_reg_entry.area_id or (dev_reg_entry.area_id if dev_reg_entry and dev_reg_entry.area_id else None)
                if area_id and self.area_registry:
                    area_entry = self.area_registry.async_get_area(area_id)
                    if area_entry:
                        area_name = area_entry.name

            description = (
                f"Entity: {entity_id}\n"
                f"Friendly Name: {friendly_name}\n"
                f"Domain: {domain}\n"
                f"State: {state}\n"
                f"Attributes: {attr_str}\n"
                f"Area: {area_name}\n"
            )

            if device_info:
                description += (
                    f"Device Info:\n"
                    f"  Manufacturer: {device_info['manufacturer']}\n"
                    f"  Model: {device_info['model']}\n"
                    f"  Device Name: {device_info['name']}\n"
                    f"  Device ID: {device_info['id']}\n"
                )

            description += (
                f"Last Changed: {entity_data.get('last_changed', 'unknown')}\n"
                f"Last Updated: {entity_data.get('last_updated', 'unknown')}\n"
                f"---\n"
            )
            entities_description.append(description)

        # Gather existing automations, truncated
        automation_entities = []
        for auto_id in self.hass.states.async_entity_ids('automation'):
            state = self.hass.states.get(auto_id)
            if state is not None:
                friendly_name = state.attributes.get('friendly_name', auto_id)
                automation_entities.append({
                    'entity_id': auto_id,
                    'friendly_name': friendly_name,
                    'state': state.state,
                    'attributes': state.attributes
                })

        automation_entities = automation_entities[:MAX_AUTOMATIONS]

        automation_description = "Existing Automations:\n"
        if automation_entities:
            for auto in automation_entities:
                auto_attr_str = str(auto['attributes'])
                if len(auto_attr_str) > MAX_ATTR_LENGTH:
                    auto_attr_str = auto_attr_str[:MAX_ATTR_LENGTH] + "...(truncated)"

                automation_description += (
                    f"Entity: {auto['entity_id']}\n"
                    f"Friendly Name: {auto['friendly_name']}\n"
                    f"State: {auto['state']}\n"
                    f"Attributes: {auto_attr_str}\n"
                    f"---\n"
                )
        else:
            automation_description += "None found.\n"

        prompt = (
            f"{self.SYSTEM_PROMPT}\n\n"
            f"Entities in your Home Assistant (randomly selected and possibly domain-filtered):\n"
            f"{''.join(entities_description)}\n\n"
            f"{automation_description}\n\n"
            f"Please suggest detailed, specific automations and improvements, "
            f"considering device and area context. Only reference the entities provided above. "
            f"Adjust triggers, conditions, and actions to refine the automations according to the provided instructions."
        )

        return prompt

    async def get_ai_suggestions(self, prompt):
        provider = self.entry.data.get(CONF_PROVIDER, "OpenAI")
        _LOGGER.debug("Using AI provider: %s", provider)

        try:
            if provider == "OpenAI":
                return await self.process_with_openai(prompt)
            elif provider == "Anthropic":
                return await self.process_with_anthropic(prompt)
            elif provider == "Google":
                return await self.process_with_google(prompt)
            elif provider == "Groq":
                return await self.process_with_groq(prompt)
            elif provider == "LocalAI":
                return await self.process_with_localai(prompt)
            elif provider == "Ollama":
                return await self.process_with_ollama(prompt)
            elif provider == "Custom OpenAI":
                return await self.process_with_custom_openai(prompt)
            elif provider == "Mistral AI":
                return await self.process_with_mistral(prompt)
            else:
                _LOGGER.error("Unknown provider: %s", provider)
                return None
        except Exception as err:
            _LOGGER.error("Error getting suggestions: %s", err)
            return None

    # ------------------------------------------------------------------------
    # Updated process_with_openai for gpt-4o / o1-preview / 03
    # ------------------------------------------------------------------------
    async def process_with_openai(self, prompt):
        try:
            api_key = self.entry.data.get(CONF_OPENAI_API_KEY)
            model = self.entry.data.get(CONF_OPENAI_MODEL, DEFAULT_MODELS["OpenAI"])
            max_tokens_conf = self.entry.data.get(CONF_MAX_TOKENS, DEFAULT_MAX_TOKENS)

            if not api_key:
                raise ValueError("OpenAI API key not configured")

            _LOGGER.debug("Making OpenAI API request with model %s", model)

            # Approximate token counting
            def count_tokens(text: str) -> int:
                return len(text) // 4  # rough approximation

            # Hard limit to avoid sending massive prompts (e.g., 32768 tokens)
            HARD_MAX = 32768
            max_tokens_used = min(max_tokens_conf, HARD_MAX)

            prompt_token_count = count_tokens(prompt)
            if prompt_token_count > max_tokens_used:
                _LOGGER.warning(
                    "Prompt is ~%d tokens, exceeding limit %d. Truncating...",
                    prompt_token_count, max_tokens_used
                )
                prompt = prompt[: max_tokens_used * 4]
                prompt_token_count = count_tokens(prompt)

            _LOGGER.debug("Prompt length after truncation: ~%d tokens", prompt_token_count)

            lower_model = model.lower()
            if lower_model in ["gpt-4o", "o1-preview", "o1", "o1-mini", "o1", "o3-mini", "o3", "gpt-4.5"]:
                data = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_completion_tokens": max_tokens_used,
                    "temperature": DEFAULT_TEMPERATURE
                }
            else:
                data = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": max_tokens_used,
                    "temperature": DEFAULT_TEMPERATURE
                }

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }

            async with self.session.post(ENDPOINT_OPENAI, headers=headers, json=data) as response:
                if response.status != 200:
                    error_text = await response.text()
                    _LOGGER.error("OpenAI API error: %s", error_text)
                    return None

                result = await response.json()
                return result["choices"][0]["message"]["content"]

        except Exception as err:
            _LOGGER.error("Error processing with OpenAI: %s", err)
            return None

    async def process_with_anthropic(self, prompt):
        try:
            api_key = self.entry.data.get(CONF_ANTHROPIC_API_KEY)
            model = self.entry.data.get(CONF_ANTHROPIC_MODEL, DEFAULT_MODELS["Anthropic"])
            max_tokens = self.entry.data.get(CONF_MAX_TOKENS, DEFAULT_MAX_TOKENS)

            if not api_key:
                raise ValueError("Anthropic API key not configured")

            _LOGGER.debug("Making Anthropic API request with model %s and max_tokens %d",
                          model, max_tokens)

            headers = {
                "Content-Type": "application/json",
                "X-API-Key": api_key,
                "anthropic-version": VERSION_ANTHROPIC
            }

            data = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": DEFAULT_TEMPERATURE
            }

            async with self.session.post(ENDPOINT_ANTHROPIC, headers=headers, json=data) as response:
                if response.status != 200:
                    error_text = await response.text()
                    _LOGGER.error("Anthropic API error: %s", error_text)
                    return None

                result = await response.json()
                return result["content"][0]["text"]

        except Exception as err:
            _LOGGER.error("Error processing with Anthropic: %s", err)
            return None

    # ------------------------------------------------------------------------
    # Updated process_with_google removing gpt-4o / o1-preview checks
    # ------------------------------------------------------------------------
    async def process_with_google(self, prompt):
        try:
            api_key = self.entry.data.get(CONF_GOOGLE_API_KEY)
            model = self.entry.data.get(CONF_GOOGLE_MODEL, DEFAULT_MODELS["Google"])
            max_tokens = min(self.entry.data.get(CONF_MAX_TOKENS, DEFAULT_MAX_TOKENS), 30720)

            if not api_key:
                raise ValueError("Google API key not configured")

            _LOGGER.debug("Making Google API request with model %s", model)

            def count_tokens(text: str) -> int:
                return len(text) // 4

            prompt_token_count = count_tokens(prompt)
            if prompt_token_count > max_tokens:
                _LOGGER.warning(
                    "Prompt is ~%d tokens, exceeding limit %d. Truncating...",
                    prompt_token_count, max_tokens
                )
                prompt = prompt[: max_tokens * 4]
                prompt_token_count = count_tokens(prompt)

            _LOGGER.debug("Prompt length after truncation: ~%d tokens", prompt_token_count)

            data = {
                "contents": [
                    {"parts": [{"text": prompt}]}
                ],
                "generationConfig": {
                    "temperature": DEFAULT_TEMPERATURE,
                    "topK": 40,
                    "topP": 0.95,
                    "maxOutputTokens": max_tokens
                }
            }

            endpoint = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
            headers = {"Content-Type": "application/json"}

            async with self.session.post(endpoint, headers=headers, json=data) as response:
                if response.status != 200:
                    error_text = await response.text()
                    _LOGGER.error("Google API error: %s", error_text)
                    return None

                result = await response.json()
                try:
                    return result["candidates"][0]["content"]["parts"][0]["text"]
                except (KeyError, IndexError) as err:
                    _LOGGER.error("Error parsing Google API response: %s", err)
                    return None

        except Exception as err:
            _LOGGER.error("Error processing with Google: %s", err)
            return None

    # ------------------------------------------------------------------------
    async def process_with_groq(self, prompt):
        try:
            api_key = self.entry.data.get(CONF_GROQ_API_KEY)
            model = self.entry.data.get(CONF_GROQ_MODEL, DEFAULT_MODELS["Groq"])
            max_tokens = self.entry.data.get(CONF_MAX_TOKENS, DEFAULT_MAX_TOKENS)

            if not api_key:
                raise ValueError("Groq API key not configured")

            _LOGGER.debug("Making Groq API request with model %s and max_tokens %d", model, max_tokens)

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }

            data = {
                "messages": [{
                    "role": "user",
                    "content": [{"type": "text", "text": prompt}]
                }],
                "model": model,
                "max_tokens": max_tokens,
                "temperature": DEFAULT_TEMPERATURE
            }

            async with self.session.post(ENDPOINT_GROQ, headers=headers, json=data) as response:
                if response.status != 200:
                    error_text = await response.text()
                    _LOGGER.error("Groq API error: %s", error_text)
                    return None

                result = await response.json()
                return result["choices"][0]["message"]["content"]

        except Exception as err:
            _LOGGER.error("Error processing with Groq: %s", err)
            return None

    async def process_with_localai(self, prompt):
        try:
            ip_address = self.entry.data.get(CONF_LOCALAI_IP_ADDRESS)
            port = self.entry.data.get(CONF_LOCALAI_PORT)
            https = self.entry.data.get(CONF_LOCALAI_HTTPS, False)
            model = self.entry.data.get(CONF_LOCALAI_MODEL, DEFAULT_MODELS["LocalAI"])
            max_tokens = self.entry.data.get(CONF_MAX_TOKENS, DEFAULT_MAX_TOKENS)

            if not ip_address or not port:
                raise ValueError("LocalAI configuration incomplete")

            protocol = "https" if https else "http"
            endpoint = ENDPOINT_LOCALAI.format(protocol=protocol, ip_address=ip_address, port=port)
            _LOGGER.debug("Making LocalAI API request to %s with model %s and max_tokens %d", endpoint, model, max_tokens)

            data = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": DEFAULT_TEMPERATURE
            }

            async with self.session.post(endpoint, json=data) as response:
                if response.status != 200:
                    error_text = await response.text()
                    _LOGGER.error("LocalAI API error: %s", error_text)
                    return None

                result = await response.json()
                return result["choices"][0]["message"]["content"]

        except Exception as err:
            _LOGGER.error("Error processing with LocalAI: %s", err)
            return None

    async def process_with_ollama(self, prompt):
        try:
            ip_address = self.entry.data.get(CONF_OLLAMA_IP_ADDRESS)
            port = self.entry.data.get(CONF_OLLAMA_PORT)
            https = self.entry.data.get(CONF_OLLAMA_HTTPS, False)
            model = self.entry.data.get(CONF_OLLAMA_MODEL, DEFAULT_MODELS["Ollama"])
            max_tokens = self.entry.data.get(CONF_MAX_TOKENS, DEFAULT_MAX_TOKENS)

            if not ip_address or not port:
                raise ValueError("Ollama configuration incomplete")

            protocol = "https" if https else "http"
            endpoint = ENDPOINT_OLLAMA.format(protocol=protocol, ip_address=ip_address, port=port)
            _LOGGER.debug("Making Ollama API request to %s with model %s and max_tokens %d", endpoint, model, max_tokens)

            data = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "options": {
                    "temperature": DEFAULT_TEMPERATURE,
                    "num_predict": max_tokens
                }
            }

            async with self.session.post(endpoint, json=data) as response:
                if response.status != 200:
                    error_text = await response.text()
                    _LOGGER.error("Ollama API error: %s", error_text)
                    return None

                result = await response.json()
                return result["message"]["content"]

        except Exception as err:
            _LOGGER.error("Error processing with Ollama: %s", err)
            return None

    async def process_with_custom_openai(self, prompt):
        try:
            endpoint = self.entry.data.get(CONF_CUSTOM_OPENAI_ENDPOINT)
            api_key = self.entry.data.get(CONF_CUSTOM_OPENAI_API_KEY)
            model = self.entry.data.get(CONF_CUSTOM_OPENAI_MODEL, DEFAULT_MODELS["Custom OpenAI"])
            max_tokens = self.entry.data.get(CONF_MAX_TOKENS, DEFAULT_MAX_TOKENS)

            if not endpoint:
                raise ValueError("Custom OpenAI endpoint not configured")

            _LOGGER.debug("Making Custom OpenAI API request to %s with model %s and max_tokens %d", endpoint, model, max_tokens)

            headers = {"Content-Type": "application/json"}
            if api_key:
                headers["Authorization"] = f"Bearer {api_key}"

            data = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": DEFAULT_TEMPERATURE
            }

            async with self.session.post(endpoint, headers=headers, json=data) as response:
                if response.status != 200:
                    error_text = await response.text()
                    _LOGGER.error("Custom OpenAI API error: %s", error_text)
                    return None

                result = await response.json()
                return result["choices"][0]["message"]["content"]

        except Exception as err:
            _LOGGER.error("Error processing with Custom OpenAI: %s", err)
            return None

    # ------------------------------------------------------------------------
    async def process_with_mistral(self, prompt):
        try:
            api_key = self.entry.data.get(CONF_MISTRAL_API_KEY)
            model = self.entry.data.get(CONF_MISTRAL_MODEL, "mistral-medium")
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": DEFAULT_TEMPERATURE,
                "max_tokens": 4096
            }
            async with self.session.post(ENDPOINT_MISTRAL, headers=headers, json=payload) as response:
                if response.status != 200:
                    error_text = await response.text()
                    _LOGGER.error("Mistral AI API error: %s", error_text)
                    return f"Error: {error_text}"
                result = await response.json()
                return result["choices"][0]["message"]["content"]
        except Exception as e:
            _LOGGER.error("Error calling Mistral AI API: %s", str(e))
            return f"Error: {str(e)}"


--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\manifest.json ---
{
  "domain": "ai_automation_suggester",
  "name": "AI Automation Suggester",
  "codeowners": ["@ITSpecialist111"],
  "config_flow": true,
  "dependencies": [],
  "documentation": "https://github.com/ITSpecialist111/ai_automation_suggester",
  "iot_class": "cloud_polling",
  "issue_tracker": "https://github.com/ITSpecialist111/ai_automation_suggester/issues",
  "requirements": [
    "openai>=1.0.0,<2.0.0",
    "anthropic>=0.8.0",
    "aiohttp>=3.8.0",
    "pyyaml>=6.0",
    "voluptuous>=0.13.1"
  ],
  "version": "1.2.7"
}


--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\sensor.py ---
# custom_components/ai_automation_suggester/sensor.py

"""Sensor platform for AI Automation Suggester."""
import logging
from homeassistant.components.sensor import SensorEntity, SensorEntityDescription
from homeassistant.helpers.update_coordinator import CoordinatorEntity, DataUpdateCoordinator
from homeassistant.helpers.entity import EntityCategory
from homeassistant.const import STATE_UNKNOWN
from homeassistant.core import callback

from .const import (
    DOMAIN,
    CONF_PROVIDER,
    PROVIDER_STATUS_CONNECTED,
    PROVIDER_STATUS_DISCONNECTED,
    PROVIDER_STATUS_ERROR,
)

_LOGGER = logging.getLogger(__name__)

SUGGESTION_SENSOR = SensorEntityDescription(
    key="suggestions",
    name="AI Automation Suggestions",
    icon="mdi:robot",
)

STATUS_SENSOR = SensorEntityDescription(
    key="status",
    name="AI Provider Status",
    icon="mdi:check-network",
    entity_category=EntityCategory.DIAGNOSTIC,
)

async def async_setup_entry(hass, entry, async_add_entities):
    """Set up the sensor platform."""
    coordinator = hass.data[DOMAIN][entry.entry_id]

    entities = [
        AISuggestionsSensor(
            coordinator=coordinator,
            entry=entry,
            description=SUGGESTION_SENSOR,
        ),
        AIProviderStatusSensor(
            coordinator=coordinator,
            entry=entry,
            description=STATUS_SENSOR,
        ),
    ]

    async_add_entities(entities, True)
    _LOGGER.debug("Sensor platform setup complete")


class AISuggestionsSensor(CoordinatorEntity, SensorEntity):
    """Sensor to display AI suggestions."""

    def __init__(self, coordinator: DataUpdateCoordinator, entry, description: SensorEntityDescription) -> None:
        """Initialize the sensor."""
        super().__init__(coordinator)
        self.entity_description = description
        self._attr_unique_id = f"{entry.entry_id}_{description.key}"
        self._attr_device_info = {
            "identifiers": {(DOMAIN, entry.entry_id)},
            "name": f"AI Automation Suggester ({entry.data.get(CONF_PROVIDER, 'unknown')})",
            "manufacturer": "Community",
            "model": entry.data.get(CONF_PROVIDER, "unknown"),
            "sw_version": "1.2.7",
        }
        self._entry = entry
        self._previous_suggestions = None
        self._attr_native_value = "No Suggestions"

    @property
    def name(self) -> str:
        """Return the name of the sensor."""
        provider = self._entry.data.get(CONF_PROVIDER, "unknown")
        return f"AI Automation Suggestions ({provider})"

    @property
    def native_value(self) -> str:
        """Return the state of the sensor."""
        if not self.coordinator.data or not self.coordinator.data.get("suggestions"):
            return "No Suggestions"

        suggestions = self.coordinator.data.get("suggestions")
        if suggestions in ["No suggestions available", "No suggestions yet"]:
            return "No Suggestions"

        if suggestions != self._previous_suggestions:
            return "New Suggestions Available"

        return "Suggestions Available"

    @property
    def extra_state_attributes(self) -> dict:
        """Return the state attributes."""
        if not self.coordinator.data:
            return {
                "suggestions": "No suggestions available",
                "last_update": None,
                "entities_processed": [],
                "provider": self._entry.data.get(CONF_PROVIDER, "unknown"),
            }

        suggestions = self.coordinator.data.get("suggestions")
        display_suggestions = suggestions if suggestions not in ["No suggestions available", "No suggestions yet"] else "No suggestions available"

        return {
            "suggestions": display_suggestions,
            "last_update": self.coordinator.data.get("last_update", None),
            "entities_processed": self.coordinator.data.get("entities_processed", []),
            "provider": self._entry.data.get(CONF_PROVIDER, "unknown"),
        }

    @property
    def available(self) -> bool:
        """Return True if entity is available."""
        return True

    @callback
    def _handle_coordinator_update(self) -> None:
        """Handle updated data from the coordinator."""
        if self.coordinator.data:
            suggestions = self.coordinator.data.get("suggestions")
            if suggestions and suggestions != self._previous_suggestions:
                self._previous_suggestions = suggestions
                self._attr_native_value = self.native_value
        self.async_write_ha_state()

    async def async_added_to_hass(self) -> None:
        """Run when entity is added to registry."""
        await super().async_added_to_hass()
        _LOGGER.debug("Suggestions sensor added to registry")


class AIProviderStatusSensor(CoordinatorEntity, SensorEntity):
    """Sensor to display provider status."""

    def __init__(self, coordinator: DataUpdateCoordinator, entry, description: SensorEntityDescription) -> None:
        """Initialize the sensor."""
        super().__init__(coordinator)
        self.entity_description = description
        self._attr_unique_id = f"{entry.entry_id}_{description.key}"
        self._attr_device_info = {
            "identifiers": {(DOMAIN, entry.entry_id)},
            "name": f"AI Automation Suggester ({entry.data.get(CONF_PROVIDER, 'unknown')})",
            "manufacturer": "Community",
            "model": entry.data.get(CONF_PROVIDER, "unknown"),
            "sw_version": "1.2.7",
        }
        self._entry = entry
        self._attr_native_value = STATE_UNKNOWN
        self._last_error = None

    @property
    def name(self) -> str:
        """Return the name of the sensor."""
        provider = self._entry.data.get(CONF_PROVIDER, "unknown")
        return f"AI Provider Status ({provider})"

    def _get_provider_status(self) -> str:
        """Determine the current status of the provider."""
        if not self.coordinator.last_update:
            return PROVIDER_STATUS_DISCONNECTED
        try:
            if (
                self.coordinator.data 
                and isinstance(self.coordinator.data, dict)
                and "suggestions" in self.coordinator.data
            ):
                return PROVIDER_STATUS_CONNECTED
            else:
                return PROVIDER_STATUS_ERROR
        except Exception as err:
            _LOGGER.error("Error getting provider status: %s", err)
            return PROVIDER_STATUS_ERROR

    @property
    def extra_state_attributes(self) -> dict:
        """Return the state attributes."""
        return {"last_error": self._last_error}

    @property
    def available(self) -> bool:
        """Return True if entity is available."""
        return True

    @callback
    def _handle_coordinator_update(self) -> None:
        """Handle updated data from the coordinator."""
        self._attr_native_value = self._get_provider_status()
        self.async_write_ha_state()

    async def async_added_to_hass(self) -> None:
        """Run when entity is added to registry."""
        await super().async_added_to_hass()
        _LOGGER.debug("Provider status sensor added to registry")



--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\services.yaml ---
generate_suggestions:
  name: Generate Suggestions
  description: "Manually trigger AI automation suggestions."
  fields:
    provider_config:
      name: Provider Configuration
      description: Which provider configuration to use (if you have multiple configured)
      required: false
      selector:
        config_entry:
          integration: ai_automation_suggester
    custom_prompt:
      name: Custom Prompt
      description: Optional custom prompt to override or enhance the default system prompt.
      required: false
      example: "Focus on energy-saving automations"  
      selector:
        text:
          multiline: true
    all_entities:
      name: Consider All Entities
      description: "If true, consider all entities instead of just new entities."
      required: false
      default: false
      selector:
        boolean: {}
    domains:
      name: Domains
      description: "List of domains to consider. If empty, consider all domains."
      required: false
      selector:
        object: {}
      default: {}
    entity_limit:
      name: Entity Limit
      description: "Maximum number of entities to consider (randomly selected from the chosen domains)."
      required: false
      default: 200
      selector:
        number:
          min: 50
          max: 1000
          mode: slider



--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\strings.json ---
{
  "title": "AI Automation Suggester",
  "config": {
    "step": {
      "user": {
        "title": "Configure AI Automation Suggester",
        "data": {
          "provider": "AI Provider"
        }
      }
    },
    "error": {
      "required": "This field is required.",
      "invalid_api_key": "The API key is invalid."
    }
  },
  "services": {
    "generate_suggestions": {
      "name": "Generate Suggestions",
      "description": "Manually trigger AI automation suggestions."
    }
  }
}



--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\custom_components\ai_automation_suggester\__init__.py ---
"""The AI Automation Suggester integration."""
import logging
from homeassistant.config_entries import ConfigEntry
from homeassistant.core import HomeAssistant, ServiceCall, callback
from homeassistant.exceptions import ConfigEntryNotReady, ServiceValidationError
from homeassistant.helpers.typing import ConfigType

from .const import (
    DOMAIN,
    PLATFORMS,
    CONF_PROVIDER,
    SERVICE_GENERATE_SUGGESTIONS,
    ATTR_PROVIDER_CONFIG,
    ATTR_CUSTOM_PROMPT,
    CONFIG_VERSION
)
from .coordinator import AIAutomationCoordinator

_LOGGER = logging.getLogger(__name__)

async def async_migrate_entry(hass: HomeAssistant, config_entry: ConfigEntry) -> bool:
    """Migrate old config entry if necessary."""
    _LOGGER.debug(f"async_migrate_entry {config_entry.version}")
    # Currently, no migration logic beyond ensuring version matches CONFIG_VERSION
    if config_entry.version < CONFIG_VERSION:
        _LOGGER.debug(f"Migrating config entry from version {config_entry.version} to {CONFIG_VERSION}")
        new_data = {**config_entry.data}
        new_data.pop('scan_frequency', None)
        new_data.pop('initial_lag_time', None)
        config_entry.version = CONFIG_VERSION
        hass.config_entries.async_update_entry(config_entry, data=new_data)
        _LOGGER.debug("Migration successful")
        return True
    return True

async def async_setup(hass: HomeAssistant, config: ConfigType) -> bool:
    """Set up the AI Automation Suggester component."""
    hass.data.setdefault(DOMAIN, {})

    async def handle_generate_suggestions(call: ServiceCall) -> None:
        """Handle the generate_suggestions service call."""
        provider_config = call.data.get(ATTR_PROVIDER_CONFIG)
        custom_prompt = call.data.get(ATTR_CUSTOM_PROMPT)
        all_entities = call.data.get("all_entities", False)
        domains = call.data.get("domains", {})
        entity_limit = call.data.get("entity_limit", 200)

        # Parse domains if provided as a string or dict
        if isinstance(domains, str):
            domains = [d.strip() for d in domains.split(',') if d.strip()]
        elif isinstance(domains, dict):
            domains = list(domains.keys())

        try:
            coordinator = None
            if provider_config:
                coordinator = hass.data[DOMAIN].get(provider_config)
            else:
                # Find first available coordinator if none specified
                for entry_id, coord in hass.data[DOMAIN].items():
                    if isinstance(coord, AIAutomationCoordinator):
                        coordinator = coord
                        break

            if coordinator is None:
                raise ServiceValidationError("No AI Automation Suggester provider configured")

            if custom_prompt:
                original_prompt = coordinator.SYSTEM_PROMPT
                coordinator.SYSTEM_PROMPT = f"{coordinator.SYSTEM_PROMPT}\n\nAdditional instructions:\n{custom_prompt}"
            else:
                original_prompt = None

            coordinator.scan_all = all_entities
            coordinator.selected_domains = domains
            coordinator.entity_limit = entity_limit

            try:
                await coordinator.async_request_refresh()
            finally:
                if original_prompt is not None:
                    coordinator.SYSTEM_PROMPT = original_prompt
                coordinator.scan_all = False
                coordinator.selected_domains = []
                coordinator.entity_limit = 200

        except KeyError:
            raise ServiceValidationError("Provider configuration not found")
        except Exception as err:
            raise ServiceValidationError(f"Failed to generate suggestions: {err}")

    # Register the service
    hass.services.async_register(
        DOMAIN,
        SERVICE_GENERATE_SUGGESTIONS,
        handle_generate_suggestions
    )

    return True

async def async_setup_entry(hass: HomeAssistant, entry: ConfigEntry) -> bool:
    """Set up AI Automation Suggester from a config entry."""
    try:
        if CONF_PROVIDER not in entry.data:
            raise ConfigEntryNotReady("Provider not specified in config")

        coordinator = AIAutomationCoordinator(hass, entry)
        hass.data[DOMAIN][entry.entry_id] = coordinator

        # Use the new async_forward_entry_setups method (plural) instead of the deprecated async_forward_entry_setup.
        await hass.config_entries.async_forward_entry_setups(entry, PLATFORMS)

        _LOGGER.debug(
            "Setup complete for %s with provider %s",
            entry.title,
            entry.data.get(CONF_PROVIDER)
        )

        entry.async_on_unload(entry.add_update_listener(async_reload_entry))

        @callback
        def handle_custom_event(event):
            _LOGGER.debug("Received custom event '%s', triggering suggestions with all_entities=True", event.event_type)
            hass.async_create_task(coordinator_request_all_suggestions())

        async def coordinator_request_all_suggestions():
            coordinator.scan_all = True
            await coordinator.async_request_refresh()
            coordinator.scan_all = False

        entry.async_on_unload(hass.bus.async_listen("ai_automation_suggester_update", handle_custom_event))

        return True

    except Exception as err:
        _LOGGER.error("Failed to setup integration: %s", err)
        raise ConfigEntryNotReady from err

async def async_unload_entry(hass: HomeAssistant, entry: ConfigEntry) -> bool:
    """Unload a config entry."""
    try:
        unload_ok = await hass.config_entries.async_unload_platforms(entry, PLATFORMS)
        if unload_ok:
            coordinator = hass.data[DOMAIN].pop(entry.entry_id)
            await coordinator.async_shutdown()
        return unload_ok
    except Exception as err:
        _LOGGER.error("Error unloading entry: %s", err)
        return False

async def async_reload_entry(hass: HomeAssistant, entry: ConfigEntry) -> None:
    """Reload config entry."""
    await async_unload_entry(hass, entry)
    await async_setup_entry(hass, entry)



--- C:\Users\graham\Documents\GitHub\ai_automation_suggester\hacs.json ---
{
  "name": "AI Automation Suggester",
  "content_in_root": false,
  "render_readme": true,
  "homeassistant": "2024.1.0",
  "hacs": "1.26.0"
}


